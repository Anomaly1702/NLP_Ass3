{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning \n",
    "- train the pre-train model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>body_text</th>\n",
       "      <th>authors</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>Language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PMC8524215</td>\n",
       "      <td>cancer prevalent cause mortality world common ...</td>\n",
       "      <td>['Muhammad  Sufyan ', 'Farah  Shahid ', 'Faiza...</td>\n",
       "      <td>implementation vaccinomics insilico approach c...</td>\n",
       "      <td>68591    One of the most common gynecologic ca...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PMC7698314</td>\n",
       "      <td>sarscov2 virus responsible covid19 disease fir...</td>\n",
       "      <td>['Enrique  Casalino ', 'Christophe  Choquet ',...</td>\n",
       "      <td>analysis emergency department visit hospital a...</td>\n",
       "      <td>813557    ED-visits and through-ED admissions ...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PMC8050998</td>\n",
       "      <td>artificial intelligence ai methodology compute...</td>\n",
       "      <td>['Martina  Gurgitano ', 'Salvatore Alessio Ang...</td>\n",
       "      <td>interventional radiology exmachina impact arti...</td>\n",
       "      <td>757155    Artificial intelligence (AI) is a br...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PMC7581283</td>\n",
       "      <td>common feature patient admitted coronavirus di...</td>\n",
       "      <td>['Ejvind Frausing Hansen ', 'Charlotte Sandau ...</td>\n",
       "      <td>automatic oxygen titration o2matic® patient ad...</td>\n",
       "      <td>942009    INTRODUCTION: Patients with coronavi...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PMC7392455</td>\n",
       "      <td>coronavirus disease covid19 caused severe acut...</td>\n",
       "      <td>['Daniel O. Griffin ', 'Alexandra  Jensen ', '...</td>\n",
       "      <td>pulmonary embolism increased level ddimer pati...</td>\n",
       "      <td>843714    We report 3 patients with coronaviru...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     paper_id                                          body_text  \\\n",
       "0  PMC8524215  cancer prevalent cause mortality world common ...   \n",
       "1  PMC7698314  sarscov2 virus responsible covid19 disease fir...   \n",
       "2  PMC8050998  artificial intelligence ai methodology compute...   \n",
       "3  PMC7581283  common feature patient admitted coronavirus di...   \n",
       "4  PMC7392455  coronavirus disease covid19 caused severe acut...   \n",
       "\n",
       "                                             authors  \\\n",
       "0  ['Muhammad  Sufyan ', 'Farah  Shahid ', 'Faiza...   \n",
       "1  ['Enrique  Casalino ', 'Christophe  Choquet ',...   \n",
       "2  ['Martina  Gurgitano ', 'Salvatore Alessio Ang...   \n",
       "3  ['Ejvind Frausing Hansen ', 'Charlotte Sandau ...   \n",
       "4  ['Daniel O. Griffin ', 'Alexandra  Jensen ', '...   \n",
       "\n",
       "                                               title  \\\n",
       "0  implementation vaccinomics insilico approach c...   \n",
       "1  analysis emergency department visit hospital a...   \n",
       "2  interventional radiology exmachina impact arti...   \n",
       "3  automatic oxygen titration o2matic® patient ad...   \n",
       "4  pulmonary embolism increased level ddimer pati...   \n",
       "\n",
       "                                            abstract Language  \n",
       "0  68591    One of the most common gynecologic ca...       en  \n",
       "1  813557    ED-visits and through-ED admissions ...       en  \n",
       "2  757155    Artificial intelligence (AI) is a br...       en  \n",
       "3  942009    INTRODUCTION: Patients with coronavi...       en  \n",
       "4  843714    We report 3 patients with coronaviru...       en  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "article_df = pd.read_csv('md_pmc_covid_articles.csv')\n",
    "# article_df.head(5)\n",
    "proc_article_df = pd.read_csv('Processed_covid_articles.csv')\n",
    "proc_article_df = proc_article_df.drop(['Unnamed: 0'], axis=1)\n",
    "proc_article_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body_text</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cancer prevalent cause mortality world common ...</td>\n",
       "      <td>68591    One of the most common gynecologic ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sarscov2 virus responsible covid19 disease fir...</td>\n",
       "      <td>813557    ED-visits and through-ED admissions ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>artificial intelligence ai methodology compute...</td>\n",
       "      <td>757155    Artificial intelligence (AI) is a br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>common feature patient admitted coronavirus di...</td>\n",
       "      <td>942009    INTRODUCTION: Patients with coronavi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>coronavirus disease covid19 caused severe acut...</td>\n",
       "      <td>843714    We report 3 patients with coronaviru...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           body_text  \\\n",
       "0  cancer prevalent cause mortality world common ...   \n",
       "1  sarscov2 virus responsible covid19 disease fir...   \n",
       "2  artificial intelligence ai methodology compute...   \n",
       "3  common feature patient admitted coronavirus di...   \n",
       "4  coronavirus disease covid19 caused severe acut...   \n",
       "\n",
       "                                            abstract  \n",
       "0  68591    One of the most common gynecologic ca...  \n",
       "1  813557    ED-visits and through-ED admissions ...  \n",
       "2  757155    Artificial intelligence (AI) is a br...  \n",
       "3  942009    INTRODUCTION: Patients with coronavi...  \n",
       "4  843714    We report 3 patients with coronaviru...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# divide the dataframe into training data and test data\n",
    "article_df_copy = proc_article_df.copy()\n",
    "train_article = article_df_copy[['body_text','abstract']].iloc[:4000]  # 4000 rows for training\n",
    "test_article = article_df_copy[['body_text','abstract']].iloc[4000:] # 500 rows for testing\n",
    "\n",
    "train_article.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = tokenizer(train_article['body_text'][0], padding=\"max_length\", truncation=True)\n",
    "temp.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "\n",
    "def tokenization(df):\n",
    "    \n",
    "    # tokenize in ids the body text and abstract\n",
    "    tokenized_ids = list()\n",
    "    # results_df ={ \n",
    "    #     'feature_input_ids': 0,\n",
    "    #     'feature_attention_mask': 0,\n",
    "    #     'feature_text': 0,\n",
    "    #     'label_input_ids': 0,\n",
    "    #     'label_attention_mask': 0,\n",
    "    #     'label_text': 0,\n",
    "    #     }\n",
    "    for i,j in zip(df['body_text'], df['abstract']):\n",
    "        f_tokenized = tokenizer(i, padding=\"max_length\", truncation=True)\n",
    "        l_tokenized = tokenizer(j, padding=\"max_length\", truncation=True)\n",
    "        results_df ={ \n",
    "        'feature_input_ids': f_tokenized['input_ids'].flatten(),\n",
    "        'feature_attention_mask': f_tokenized['attention_mask'],\n",
    "        'feature_text': i,\n",
    "        'label_input_ids': l_tokenized['input_ids'].flatten(),\n",
    "        'label_attention_mask': l_tokenized['attention_mask'],\n",
    "        'label_text': j\n",
    "        }\n",
    "        \n",
    "        tokenized_ids.append(results_df)\n",
    "    return tokenized_ids\n",
    "\n",
    "# list of dictionaries, each dictionary is a tokenized article with 'input_ids', 'token_type_ids', 'attention_mask'\n",
    "\n",
    "train_X_encode_ids = tokenization(train_article)\n",
    "test_X_encode_ids = tokenization(test_article)\n",
    "\n",
    "# train_y_encode_ids = tokenization(train_article, \"abstract\")\n",
    "# test_y_encode_ids = tokenization(test_article, \"abstract\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['feature_input_ids', 'feature_attention_mask', 'feature_text', 'label_input_ids', 'label_attention_mask', 'label_text'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X_encode_ids[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = train_X_encode_ids[0:1000]\n",
    "small_eval_dataset = test_X_encode_ids[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(small_train_dataset, shuffle=True, batch_size=8)\n",
    "eval_dataloader = DataLoader(small_eval_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import BertForSequenceClassification\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from transformers import get_scheduler\n",
    "import tqdm\n",
    "\n",
    "l_rate = 1e-5\n",
    "epochs = 2\n",
    "train_steps = epochs * len(train_dataloader)\n",
    "#Get transformaer specific model later\n",
    "t_model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "optimizer = Adam(params = t_model.parameters(), lr=l_rate)\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=train_steps\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "mps_device = torch.device(\"cpu\")\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print(x)\n",
    "else:\n",
    "    print(\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/kamanbeckypang/Documents/year2_trim_1/NLP/nlp_assign_3/fine_tuning.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 29>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kamanbeckypang/Documents/year2_trim_1/NLP/nlp_assign_3/fine_tuning.ipynb#W0sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m             optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kamanbeckypang/Documents/year2_trim_1/NLP/nlp_assign_3/fine_tuning.ipynb#W0sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m             \u001b[39m#progress_bar.update(1)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/kamanbeckypang/Documents/year2_trim_1/NLP/nlp_assign_3/fine_tuning.ipynb#W0sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m train(epochs, t_model, optimizer,train_dataloader)\n",
      "\u001b[1;32m/Users/kamanbeckypang/Documents/year2_trim_1/NLP/nlp_assign_3/fine_tuning.ipynb Cell 13\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs, model, optimizer, train_dataloader)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kamanbeckypang/Documents/year2_trim_1/NLP/nlp_assign_3/fine_tuning.ipynb#W0sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m model\u001b[39m.\u001b[39mtrain() \u001b[39m# the model is in training mode, will consider evething like batch normalization and dropout\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kamanbeckypang/Documents/year2_trim_1/NLP/nlp_assign_3/fine_tuning.ipynb#W0sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m train_dataloader:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/kamanbeckypang/Documents/year2_trim_1/NLP/nlp_assign_3/fine_tuning.ipynb#W0sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     batch \u001b[39m=\u001b[39m {k: v\u001b[39m.\u001b[39mto(device) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m batch\u001b[39m.\u001b[39mitems()}\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kamanbeckypang/Documents/year2_trim_1/NLP/nlp_assign_3/fine_tuning.ipynb#W0sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mbatch)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kamanbeckypang/Documents/year2_trim_1/NLP/nlp_assign_3/fine_tuning.ipynb#W0sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     loss \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mloss\n",
      "\u001b[1;32m/Users/kamanbeckypang/Documents/year2_trim_1/NLP/nlp_assign_3/fine_tuning.ipynb Cell 13\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kamanbeckypang/Documents/year2_trim_1/NLP/nlp_assign_3/fine_tuning.ipynb#W0sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m model\u001b[39m.\u001b[39mtrain() \u001b[39m# the model is in training mode, will consider evething like batch normalization and dropout\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kamanbeckypang/Documents/year2_trim_1/NLP/nlp_assign_3/fine_tuning.ipynb#W0sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m train_dataloader:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/kamanbeckypang/Documents/year2_trim_1/NLP/nlp_assign_3/fine_tuning.ipynb#W0sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     batch \u001b[39m=\u001b[39m {k: v\u001b[39m.\u001b[39;49mto(device) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m batch\u001b[39m.\u001b[39mitems()}\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kamanbeckypang/Documents/year2_trim_1/NLP/nlp_assign_3/fine_tuning.ipynb#W0sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mbatch)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kamanbeckypang/Documents/year2_trim_1/NLP/nlp_assign_3/fine_tuning.ipynb#W0sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     loss \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mloss\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "def train(epochs, model, optimizer,train_dataloader):\n",
    "    train_loss_ls = []\n",
    "    train_accuracy_ls = []\n",
    "    val_loss_ls = []\n",
    "    val_accuracy_ls = []\n",
    "    model = model.to(mps_device)\n",
    "    #progress_bar = tqdm(range(train_steps))\n",
    "    '''\n",
    "     This is the training section\n",
    "    '''\n",
    "    \n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "        \n",
    "        training_loss = 0\n",
    "        training_correct = 0\n",
    "        model.train() # the model is in training mode, will consider evething like batch normalization and dropout\n",
    "            \n",
    "        for batch in train_dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            #progress_bar.update(1)\n",
    "\n",
    "train(epochs, t_model, optimizer,train_dataloader)\n",
    "\n",
    "            \n",
    "        \n",
    "        # for i, data in enumerate(train_dataloader, 0): # each data is one batch\n",
    "        #     # get the inputs; data is a list of [inputs, labels]\n",
    "        #     inputs, labels = data\n",
    "        #     inputs, labels = inputs.to(mps_device), labels.to(mps_device)\n",
    "\n",
    "        #     # zero the parameter gradients\n",
    "        #     optimizer.zero_grad()\n",
    "\n",
    "        #     # forward + backward + optimize\n",
    "        #     outputs = model(inputs)\n",
    "        #     predicted = outputs.argmax(1)# index of max probability out of 10 classes\n",
    "\n",
    "        #     loss = criterion(outputs, labels)\n",
    "        #     loss.backward()\n",
    "        #     optimizer.step()\n",
    "\n",
    "        #     # print statistics\n",
    "        #     training_loss += loss.item()\n",
    "        #     training_correct += (predicted == labels).sum().item()\n",
    "\n",
    "#         '''\n",
    "#         This is the validation section\n",
    "# #         '''\n",
    "#         val_loss = 0\n",
    "#         val_correct = 0\n",
    "# #         val_total = 0\n",
    "#         model.eval()\n",
    "#         with torch.no_grad(): # do not perform any calculation  \n",
    "#             for data in test_dataloader:\n",
    "#                 inputs, labels = data\n",
    "#                 inputs, labels = inputs.to(device), labels.to(device)\n",
    "#                 outputs = model(inputs)\n",
    "#                 predicted = outputs.argmax(1)\n",
    "#                 loss = criterion(outputs, labels)\n",
    "#                 val_loss += loss.item()\n",
    "# #                 val_total += labels.size(0)\n",
    "#                 val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "# #         print('Val Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))\n",
    "\n",
    "#         training_loss = training_loss/(len(trainloader_resnet)) # this is consider as batch\n",
    "#         train_acc = (training_correct / len(trainloader_resnet.dataset)) # this consider as all simple\n",
    "#         val_loss = val_loss / len(validloader_resnet)\n",
    "#         val_acc = (val_correct/len(validloader_resnet.dataset))\n",
    "        \n",
    "#         # save the loss and accuracy in the list\n",
    "#         train_loss_ls.append(training_loss)\n",
    "#         train_accuracy_ls.append(train_acc)\n",
    "#         val_loss_ls.append(val_loss)\n",
    "#         val_accuracy_ls.append(val_acc)\n",
    "#         print(\"epoch\",epoch)\n",
    "#         print(\"train_accuracy: \",train_acc,\"val_accuracy: \",val_acc,\"train_loss: \",training_loss, \"val_loss:\", val_loss)\n",
    "\n",
    "#     print('Finished Training')\n",
    "#     return (train_accuracy_ls,train_loss_ls,val_accuracy_ls,val_loss_ls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "model.eval()\n",
    "\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "metric.compute()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
